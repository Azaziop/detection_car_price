"""
Airflow DAG pour le pipeline MLOps CarPricePredictor
Orchestration de l'entraÃ®nement, Ã©valuation et promotion des modÃ¨les
"""
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.email import EmailOperator
from airflow.sensors.filesystem import FileSensor
from datetime import datetime, timedelta
import sys
import os

# Ajouter le chemin du projet au PYTHONPATH
project_path = '/opt/airflow/project'
sys.path.insert(0, project_path)

# Configuration par dÃ©faut du DAG
default_args = {
    'owner': 'data-science-team',
    'depends_on_past': False,
    'start_date': datetime(2026, 2, 14),
    'email': ['alerts@example.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# DÃ©finition du DAG
dag = DAG(
    'car_price_predictor_pipeline',
    default_args=default_args,
    description='Pipeline ML complet pour la prÃ©diction de prix de voitures',
    schedule_interval='@weekly',  # ExÃ©cution hebdomadaire
    catchup=False,
    tags=['machine-learning', 'mlflow', 'car-price'],
)


def check_data_quality(**context):
    """VÃ©rifier la qualitÃ© des donnÃ©es avant l'entraÃ®nement"""
    import pandas as pd
    import mlflow
    
    print("ðŸ“Š VÃ©rification de la qualitÃ© des donnÃ©es...")
    
    # Chargement des donnÃ©es
    data_path = os.path.join(project_path, 'data/raw/avito_car_dataset_ALL.csv')
    df = pd.read_csv(data_path, encoding='latin1')
    
    # Calculs de qualitÃ©
    total_rows = len(df)
    missing_values = df.isnull().sum().sum()
    missing_percentage = (missing_values / (total_rows * len(df.columns))) * 100
    
    # VÃ©rifications
    quality_checks = {
        'total_rows': total_rows,
        'missing_percentage': missing_percentage,
        'columns': list(df.columns),
        'data_ok': total_rows > 1000 and missing_percentage < 50
    }
    
    print(f"âœ… Lignes: {total_rows}")
    print(f"âœ… Valeurs manquantes: {missing_percentage:.2f}%")
    
    # Sauvegarder dans XCom pour les tÃ¢ches suivantes
    context['ti'].xcom_push(key='data_quality', value=quality_checks)
    
    if not quality_checks['data_ok']:
        raise ValueError("âŒ DonnÃ©es insuffisantes pour l'entraÃ®nement")
    
    return quality_checks


def train_model(**context):
    """EntraÃ®ner le modÃ¨le avec MLflow"""
    import mlflow
    from scripts.train_with_mlflow import CarPricePipeline
    
    print("ðŸš€ DÃ©marrage de l'entraÃ®nement du modÃ¨le...")
    
    # Configuration MLflow - utiliser l'adresse IP de l'hÃ´te Docker
    # Dans le rÃ©seau Docker, utiliser le nom de service fonctionne normalement
    # mais Ã  cause de la vÃ©rification Werkzeug, on utilise l'IP directement
    import socket
    try:
        mlflow_ip = socket.gethostbyname('mlflow')
        mlflow_uri = f"http://{mlflow_ip}:5000"
    except:
        mlflow_uri = "http://mlflow:5000"
    
    mlflow.set_tracking_uri(mlflow_uri)
    print(f"ðŸ“¡ MLflow URI: {mlflow_uri}")
    mlflow.set_experiment("car_price_prediction")
    
    # RÃ©cupÃ©rer les infos de qualitÃ© des donnÃ©es
    data_quality = context['ti'].xcom_pull(key='data_quality', task_ids='check_data_quality')
    if data_quality:
        print(f"ðŸ“Š DonnÃ©es validÃ©es: {data_quality['total_rows']} lignes")
    else:
        print("âš ï¸  Pas de donnÃ©es de qualitÃ© disponibles (test manuel?)")
    
    # EntraÃ®nement
    with mlflow.start_run(run_name=f"airflow_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
        # Tag pour identifier les runs Airflow
        mlflow.set_tag("pipeline", "airflow")
        mlflow.set_tag("trigger", "scheduled")
        
        # Chemin absolu vers params.yaml
        params_path = os.path.join(project_path, 'params.yaml')
        pipeline = CarPricePipeline(params_file=params_path, base_dir=project_path)
        
        # Load and prepare data with absolute path
        data_path = os.path.join(project_path, 'data/raw/avito_car_dataset_ALL.csv')
        df = pipeline.load_data(filepath=data_path)
        df_clean = pipeline.preprocess_data(df)
        X_scaled, y_scaled, feature_names = pipeline.prepare_features(df_clean)
        
        # Train model (handles train/test split and evaluation internally)
        X_train, X_test, y_train, y_test = pipeline.train_model(X_scaled, y_scaled)
        
        # Save model artifacts
        pipeline.save_model()
        
        # Get run ID
        run_id = mlflow.active_run().info.run_id
        
        print(f"âœ… ModÃ¨le entraÃ®nÃ© - Run ID: {run_id}")
        
        # Sauvegarder le run ID dans XCom
        context['ti'].xcom_push(key='run_id', value=run_id)
    
    return run_id


def evaluate_model(**context):
    """Ã‰valuer le modÃ¨le et dÃ©cider de la promotion"""
    import mlflow
    import socket
    from mlflow.tracking import MlflowClient
    
    print("ðŸ” Ã‰valuation du modÃ¨le...")
    
    # Configuration MLflow avec DNS resolution
    mlflow_uri = f"http://{socket.gethostbyname('mlflow')}:5000"
    mlflow.set_tracking_uri(mlflow_uri)
    
    # RÃ©cupÃ©rer le run_id
    run_id = context['ti'].xcom_pull(key='run_id', task_ids='train_model')
    print(f"ðŸ“¡ Run ID: {run_id}")
    
    # RÃ©cupÃ©rer les mÃ©triques depuis MLflow
    client = MlflowClient()
    run = client.get_run(run_id)
    
    r2_score = run.data.metrics.get('test_r2', 0.0)
    rmse = run.data.metrics.get('test_rmse', 999999)
    mae = run.data.metrics.get('test_mae', 999999)
    
    # CritÃ¨res de qualitÃ©
    MINIMUM_R2 = 0.70  # RÂ² minimum acceptable
    MAXIMUM_RMSE = 1.0  # RMSE maximum acceptable (scaled)
    
    print(f"ðŸ“Š RÂ² Score: {r2_score:.4f} (min: {MINIMUM_R2})")
    print(f"ðŸ“Š RMSE: {rmse:.4f} (max: {MAXIMUM_RMSE})")
    print(f"ðŸ“Š MAE: {mae:.4f}")
    
    # DÃ©cision
    is_promotable = r2_score >= MINIMUM_R2 and rmse <= MAXIMUM_RMSE
    
    evaluation_result = {
        'run_id': run_id,
        'r2_score': r2_score,
        'rmse': rmse,
        'mae': mae,
        'is_promotable': is_promotable,
        'evaluation_date': datetime.now().isoformat()
    }
    
    if is_promotable:
        print("âœ… ModÃ¨le Ã©ligible pour la promotion!")
    else:
        print("âš ï¸ ModÃ¨le ne satisfait pas les critÃ¨res de qualitÃ©")
    
    context['ti'].xcom_push(key='evaluation_result', value=evaluation_result)
    
    return evaluation_result


def promote_to_staging(**context):
    """Promouvoir le modÃ¨le vers Staging"""
    import mlflow
    import socket
    from mlflow.tracking import MlflowClient
    
    print("ðŸ“¦ Promotion du modÃ¨le vers Staging...")
    
    # Configuration MLflow avec DNS resolution
    mlflow_uri = f"http://{socket.gethostbyname('mlflow')}:5000"
    mlflow.set_tracking_uri(mlflow_uri)
    client = MlflowClient()
    
    # RÃ©cupÃ©rer les rÃ©sultats d'Ã©valuation
    eval_result = context['ti'].xcom_pull(key='evaluation_result', task_ids='evaluate_model')
    
    if not eval_result['is_promotable']:
        print("âš ï¸ ModÃ¨le non Ã©ligible, promotion annulÃ©e")
        return "skipped"
    
    run_id = eval_result['run_id']
    model_name = "CarPricePredictor"
    
    try:
        # Trouver la version du modÃ¨le pour ce run_id
        versions = client.search_model_versions(f"name='{model_name}'")
        model_version_obj = None
        for v in versions:
            if v.run_id == run_id:
                model_version_obj = v
                break
        
        if not model_version_obj:
            raise ValueError(f"Aucune version trouvÃ©e pour run_id {run_id}")
        
        version_number = str(model_version_obj.version)
        print(f"ðŸ“¦ Version trouvÃ©e: {version_number}")
        
        # Promouvoir vers Staging
        client.transition_model_version_stage(
            name=model_name,
            version=version_number,
            stage="Staging",
            archive_existing_versions=False
        )
        
        print(f"âœ… ModÃ¨le promu vers Staging - Version {version_number}")
        
        context['ti'].xcom_push(key='model_version', value=version_number)
        
        return version_number
        
    except Exception as e:
        print(f"âŒ Erreur lors de la promotion: {str(e)}")
        raise


def validate_staging_model(**context):
    """Valider le modÃ¨le en Staging avant production"""
    import mlflow
    import socket
    from mlflow.tracking import MlflowClient
    
    print("ðŸ§ª Validation du modÃ¨le en Staging...")
    
    # Configuration MLflow avec DNS resolution
    mlflow_uri = f"http://{socket.gethostbyname('mlflow')}:5000"
    mlflow.set_tracking_uri(mlflow_uri)
    client = MlflowClient()
    
    model_name = "CarPricePredictor"
    model_version = context['ti'].xcom_pull(key='model_version', task_ids='promote_to_staging')
    
    if not model_version:
        print("âš ï¸ Aucune version Ã  valider (modÃ¨le non promu)")
        return "skipped"
    
    print(f"ðŸ“¦ Validation de la version {model_version}")
    
    # VÃ©rifier que le modÃ¨le existe
    try:
        model_info = client.get_model_version(model_name, model_version)
        print(f"âœ… ModÃ¨le trouvÃ©: {model_info.name} v{model_info.version}")
        print(f"   Stage: {model_info.current_stage}")
        print(f"   Run ID: {model_info.run_id}")
        
        # Validation rÃ©ussie
        validation_result = {
            'model_name': model_name,
            'model_version': model_version,
            'stage': model_info.current_stage,
            'validation_passed': True,
            'validation_date': datetime.now().isoformat()
        }
        
        context['ti'].xcom_push(key='validation_result', value=validation_result)
        
        print("âœ… Validation du modÃ¨le Staging rÃ©ussie!")
        return validation_result
        
    except Exception as e:
        print(f"âŒ Erreur lors de la validation: {str(e)}")
        raise
    validation_checks = {
        'model_loaded': True,
        'predictions_valid': all(predictions > 0),
        'predictions_reasonable': all(predictions < 1000000),  # Prix < 1M
        'model_version': model_version,
        'validation_passed': True
    }
    
    print(f"âœ… Validation rÃ©ussie - Version {model_version}")
    print(f"ðŸ“Š PrÃ©dictions test: {predictions[:3]}")
    
    context['ti'].xcom_push(key='validation_checks', value=validation_checks)
    
    return validation_checks


def promote_to_production(**context):
    """Promouvoir le modÃ¨le vers Production"""
    import sys
    sys.path.insert(0, '/opt/airflow/project')
    
    import mlflow
    from mlflow.tracking import MlflowClient
    import socket
    
    print("ðŸš€ Promotion du modÃ¨le vers Production...")
    
    # Configuration MLflow avec rÃ©solution DNS
    mlflow_uri = f"http://{socket.gethostbyname('mlflow')}:5000"
    mlflow.set_tracking_uri(mlflow_uri)
    client = MlflowClient()
    
    # RÃ©cupÃ©rer la version du modÃ¨le
    model_version = context['ti'].xcom_pull(key='model_version', task_ids='promote_to_staging')
    if not model_version:
        print("âš ï¸ Aucune version Ã  promouvoir (modÃ¨le non trouvÃ©)")
        return "skipped"
    
    validation = context['ti'].xcom_pull(key='validation_result', task_ids='validate_staging_model')
    if validation and not validation.get('validation_passed', True):
        print("âš ï¸ Validation Ã©chouÃ©e, promotion vers Production annulÃ©e")
        return "skipped"
    
    model_name = "CarPricePredictor"
    
    try:
        # Archiver les versions Production actuelles
        client.transition_model_version_stage(
            name=model_name,
            version=model_version,
            stage="Production",
            archive_existing_versions=True
        )
        
        print(f"âœ… ModÃ¨le promu vers Production - Version {model_version}")
        
        return {
            'model_name': model_name,
            'version': model_version,
            'stage': 'Production',
            'promotion_date': datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"âŒ Erreur lors de la promotion Production: {str(e)}")
        raise


def send_pipeline_report(**context):
    """GÃ©nÃ©rer et envoyer un rapport du pipeline"""
    import json
    import numpy as np
    
    print("ðŸ“§ GÃ©nÃ©ration du rapport de pipeline...")
    
    def convert_to_json_serializable(obj):
        """Convertir les types numpy en types Python natifs"""
        if isinstance(obj, dict):
            return {k: convert_to_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_json_serializable(item) for item in obj]
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        else:
            return obj
    
    # RÃ©cupÃ©rer toutes les informations
    data_quality = context['ti'].xcom_pull(key='data_quality', task_ids='check_data_quality')
    run_id = context['ti'].xcom_pull(key='run_id', task_ids='train_model')
    metrics = context['ti'].xcom_pull(key='metrics', task_ids='train_model')
    eval_result = context['ti'].xcom_pull(key='evaluation_result', task_ids='evaluate_model')
    
    # CrÃ©er le rapport avec conversion des types numpy
    report = convert_to_json_serializable({
        'pipeline_date': datetime.now().isoformat(),
        'data_quality': data_quality,
        'training': {
            'run_id': run_id,
            'metrics': metrics
        },
        'evaluation': eval_result,
        'status': 'SUCCESS'
    })
    
    # Sauvegarder le rapport
    report_path = os.path.join(project_path, 'reports', f"pipeline_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    os.makedirs(os.path.dirname(report_path), exist_ok=True)
    
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"âœ… Rapport sauvegardÃ©: {report_path}")
    print(f"ðŸ“Š Status: {report['status']}")
    if metrics and 'r2_score' in metrics:
        print(f"ðŸ“Š RÂ² Score: {metrics['r2_score']:.4f}")
    
    return report


# ========================================
# DÃ©finition des tÃ¢ches
# ========================================

# TÃ¢che 1: VÃ©rification de la qualitÃ© des donnÃ©es
check_data = PythonOperator(
    task_id='check_data_quality',
    python_callable=check_data_quality,
    dag=dag,
)

# TÃ¢che 2: EntraÃ®nement du modÃ¨le
train = PythonOperator(
    task_id='train_model',
    python_callable=train_model,
    dag=dag,
)

# TÃ¢che 3: Ã‰valuation du modÃ¨le
evaluate = PythonOperator(
    task_id='evaluate_model',
    python_callable=evaluate_model,
    dag=dag,
)

# TÃ¢che 4: Promotion vers Staging
staging = PythonOperator(
    task_id='promote_to_staging',
    python_callable=promote_to_staging,
    dag=dag,
)

# TÃ¢che 5: Validation du modÃ¨le en Staging
validate = PythonOperator(
    task_id='validate_staging_model',
    python_callable=validate_staging_model,
    dag=dag,
)

# TÃ¢che 6: Promotion vers Production
production = PythonOperator(
    task_id='promote_to_production',
    python_callable=promote_to_production,
    dag=dag,
)

# TÃ¢che 7: Rapport final
report = PythonOperator(
    task_id='send_pipeline_report',
    python_callable=send_pipeline_report,
    dag=dag,
)

# ========================================
# DÃ©finition du flux de tÃ¢ches
# ========================================

check_data >> train >> evaluate >> staging >> validate >> production >> report
